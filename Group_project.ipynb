{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRGXi5ZvJS3y"
   },
   "source": [
    "## COMP5721M: Programming for Data Science\n",
    "\n",
    "## Group project (Coursework 2): Data Analysis Project\n",
    "\n",
    "\n",
    "# Predictive Analysis of Heart Attack Risk Using Machine Learning Algorithms\n",
    "\n",
    "Group member names\n",
    "* Rajvi Rajesh Jagani, qjth6630@leeds.ac.uk\n",
    "* Stan Kilburn, kmgc7321@leeds.ac.uk\n",
    "* Zaid Rupani, rtgw1267@leeds.ac.uk\n",
    "* Bhargav Kumar Nath, vnnh7247@leeds.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ozr_MIPiJS32"
   },
   "source": [
    "# Project Plan\n",
    "\n",
    "## The Data (10 marks)\n",
    "### Dataset Overview\n",
    "The **Heart Failure Prediction** dataset, available on Kaggle, is a critical resource for researchers and practitioners in the healthcare field, focusing on cardiovascular diseases (CVDs), which remain the leading cause of death worldwide, claiming approximately 17.9 million lives annually. Among these, heart failure is a prevalent outcome that necessitates early detection and intervention, especially for individuals at high cardiovascular risk due to factors like hypertension, diabetes, and hyperlipidaemia. This dataset comprises 11 features, making it a valuable tool for developing machine learning models to predict heart disease.\n",
    "\n",
    "### Context and Importance\n",
    "Given the alarming statistics surrounding CVDs—where four out of five deaths result from heart attacks and strokes—it is vital to leverage advanced analytical techniques for early diagnosis and management. The Heart Failure Prediction dataset facilitates this need by providing comprehensive data that can aid in identifying individuals who may be at risk of heart failure, thereby enhancing preventive healthcare measures.\n",
    "\n",
    "### Attribute Information\n",
    "The dataset includes the following key attributes:\n",
    "\n",
    "- **Age**: Age of the patient in years.\n",
    "- **Sex**: Gender of the patient (M: Male, F: Female).\n",
    "- **ChestPainType**: Classification of chest pain (e.g., Typical Angina, Atypical Angina, Non-Anginal Pain, Asymptomatic).\n",
    "- **RestingBP**: Resting blood pressure in mm Hg.\n",
    "- **Cholesterol**: Serum cholesterol levels in mg/dl.\n",
    "- **FastingBS**: Fasting blood sugar levels (1: if fasting blood sugar > 120 mg/dl, 0: otherwise).\n",
    "- **RestingECG**: Results of a resting electrocardiogram (Normal, ST abnormality, LVH).\n",
    "- **MaxHR**: Maximum heart rate achieved (numerical value between 60 and 202).\n",
    "- **ExerciseAngina**: Indicates exercise-induced angina (Y: Yes, N: No).\n",
    "- **Oldpeak**: Measurement of ST segment depression.\n",
    "- **ST_Slope**: Slope of the peak exercise ST segment (Up, Flat, Down).\n",
    "- **HeartDisease**: Output class indicating the presence of heart disease (1: heart disease, 0: normal).\n",
    "\n",
    "### Source and Composition\n",
    "The dataset is notable for being a comprehensive amalgamation of five independent heart disease datasets, which collectively offer a robust sample size. These datasets include:\n",
    "\n",
    "- Cleveland (303 observations)\n",
    "- Hungarian (294 observations)\n",
    "- Switzerland (123 observations)\n",
    "- Long Beach VA (200 observations)\n",
    "- Stalog (Heart) Data Set (270 observations)\n",
    "\n",
    "In total, the combined dataset contains **1190 observations**, with **272 duplicates** removed, resulting in a final dataset of **918 observations**. Each of these individual datasets is accessible through the UCI Machine Learning Repository, ensuring transparency and facilitating further research.\n",
    "\n",
    "### Citation and Acknowledgements\n",
    "The dataset was created with contributions from various medical institutions and professionals, emphasizing the collaborative effort behind its compilation. For further reference, the dataset can be cited as follows:\n",
    "\n",
    "fedesoriano. (September 2021). Heart Failure Prediction Dataset. Retrieved [Date Retrieved] from [Kaggle Link](https://www.kaggle.com/fedesoriano/heart-failure-prediction).\n",
    "\n",
    "By utilizing this dataset, we can contribute significantly to understanding and predicting heart failure, ultimately advancing patient care and treatment outcomes.\n",
    "\n",
    "\n",
    "\n",
    "## Project Aim and Objectives (5 marks)\n",
    "\n",
    "The aim of this data analysis project is to develop and compare 4 different Machine Learning algorithms that will use measurements of different factors relating to a patient's health, to assign a label 0 or 1, indicating whether or not that patient has heart disease.\n",
    "\n",
    "We hope to provide several reliable, data-driven approaches to predicting heart disease, that could be used confidently by healthcare professionals. We also discuss the limitations of our analyses, which presents opportunities to further optimise our results in the future.\n",
    "\n",
    "We will provide a detailed comparison of the performance of each machine learning algorithm, using several different metrics and visualisations. Beyond this we will also consider the context, and explore the advantages and disadvantages of using each model within this particular healthcare field.\n",
    "\n",
    "Each algorithm will train a model using a subset of the original data set, and then we will measure performance on a smaller test data set, that is unseen to the model. We will ensure fairness in our project comparisons through using identical testing and training data sets - this will allow us to really see the key differences between models.\n",
    "\n",
    "\n",
    "### Specific Objectives\n",
    "\n",
    "\n",
    "1. **Objective 1:** Clean and preprocess the dataset by handling missing values, encoding categorical features, and normalizing numerical values where necessary. This will ensure the dataset is well-prepared for accurate and reliable model training.\n",
    "\n",
    "2. **Objective 2:** Build a logistic regression model using sci-kit learn's LogisticRegression.\n",
    "\n",
    "3. **Objective 3:** Implement multiple machine learning models, including logistic regression, decision trees, and random forests, to classify patients based on heart disease risk. Each model will be trained and tested to identify the most effective algorithm for accurate prediction.\n",
    "\n",
    "4. **Objective 4:** Evaluate and compare model performance using accuracy, precision, recall, and F1 score. This will help in identifying the best model, optimizing it further if necessary, and assessing its potential for practical application in healthcare settings.\n",
    "\n",
    "5. **Objective 5** Compare machine learning algorithms\n",
    "\n",
    "## System Design (5 marks)\n",
    "\n",
    "_Describe your code in terms of the\n",
    "following two sections._\n",
    "\n",
    "### Architecture\n",
    "\n",
    "_Typically this would be  a pipeline in which data goes through several\n",
    "stages of transformation and analysis, but other architectures are possible.\n",
    "This does not need to be particularly complicated. A simple diagram with\n",
    "100-150 words of explanation would\n",
    "be a good way to present your architecture._\n",
    "  \n",
    "### Processing Modules and Algorithms\n",
    "\n",
    "_Briefly list and describe the most significant computational components of your system and the algorithms you will use to implement them.\n",
    "This could include things like:_\n",
    "\n",
    "* _cleaning the data by removing outliers_\n",
    "* _combining different datasets_\n",
    "* _converting samples to a special representaion (such as feature vectors)_\n",
    "* _constructing a special data-structure (such as a decision tree)_\n",
    "* _running some kind of analysis_\n",
    "\n",
    "_Your list can be presented in similar form to the one just given,\n",
    "but should include a brief\n",
    "but more specific description of the components and/or algorithms.\n",
    "Probably three or four components is sufficient for most projects, but\n",
    "you may want to have more._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlxO1EgSJS33"
   },
   "source": [
    "# Data exploration\n",
    "\n",
    "In this section we will explore the dataset using various libraries like numpy and pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDbH2ms9JS33"
   },
   "outputs": [],
   "source": [
    "# importing necessary libraries with the import statement\n",
    "# numPy provides efficient numerical operations on arrays, while pandas offers data structures and tools for data manipulation.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "bzXgv0XEJS35",
    "outputId": "64cf7945-0b79-41ed-89a8-7da3d1ee9493"
   },
   "outputs": [],
   "source": [
    "# Read the CSV file 'heart.csv' into a Pandas DataFrame\n",
    "df = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "# Randomly sample 10 rows from the DataFrame and display them. We made it random for more unbiased and representative view compared to head()\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KyeVKBjhJS36",
    "outputId": "afcc7b86-27c2-465c-bf0d-9182c8164de5"
   },
   "outputs": [],
   "source": [
    "# Overview of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxMW_B4ZJS36"
   },
   "source": [
    "The DataFrame df has 918 rows and 12 columns. It contains both numerical and categorical data, and there are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "qmD-jrV_JS37",
    "outputId": "caa50570-9710-4967-abb4-1e8574e1de16"
   },
   "outputs": [],
   "source": [
    "# Summary for numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "g9uxrMugJS37",
    "outputId": "635dc6a4-6566-4d80-bd04-f7e56dcf455f"
   },
   "outputs": [],
   "source": [
    "# Summary for categorical columns\n",
    "df.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7Hfh6fiJS38",
    "outputId": "32106d2d-e4b8-4f21-a5bf-458961fd1bea"
   },
   "outputs": [],
   "source": [
    "# Checking for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "\n",
    "print(f\"Number of rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGi_2vmKJS38"
   },
   "source": [
    "It shows we don't have any duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T423hiPpJS38"
   },
   "source": [
    "## Data distribution and value counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoBlGexrJS38",
    "outputId": "fb6d78ad-baef-41ad-fa86-10a2bde5533d"
   },
   "outputs": [],
   "source": [
    "# View unique values and their counts for categorical columns\n",
    "for col in df.select_dtypes(include=object).columns:\n",
    "    print(f\"Value counts for {col}:\")\n",
    "    print(df[col].value_counts())\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Check distribution of the target variable\n",
    "print(\"Target value distribution\")\n",
    "print(df[\"HeartDisease\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qXWbTUvJS39"
   },
   "source": [
    "The dataset is imbalanced, with more instances of the majority class (HeartDisease=1). Most individuals are male, and the majority experience typical chest pain (ASY). Resting ECG results vary, with 'Normal' being the most common. Exercise-induced angina is present in a significant portion of individuals. The ST slope is predominantly flat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t-QRYeHJS39"
   },
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRe1BGjtJS39"
   },
   "source": [
    "Looking at the histograms, the data reveals typical patterns for a heart disease study. The age distribution shows most patients are middle-aged (50-65 years), with resting blood pressure typically ranging between 120-160 mmHg. Cholesterol levels show some variation with multiple peaks between 200-300 mg/dL. Most vital signs like maximum heart rate follow expected normal distributions, while binary indicators like fasting blood sugar show clear categorical splits. Overall, these distributions suggest a representative sample for studying cardiovascular health factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYm2pY7aJS39"
   },
   "outputs": [],
   "source": [
    "def plot_feature_distributions(df, num_cols=4):\n",
    "    \"\"\"\n",
    "    Generates distribution plots for numeric features in a DataFrame using seaborn.\n",
    "\n",
    "    This function creates a grid of histogram plots with kernal density estimation\n",
    "    for all numeric columns in the input DataFrame. The plots are arranged in a\n",
    "    configurable grid layout with customizable spacing and formatting.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame containing the data to visualize\n",
    "        num_cols (int, optional): Number of columns in the plot grid.\n",
    "\n",
    "    Returns:\n",
    "        None. It displays the plot using matplotliv.pyplot.show()\n",
    "    \"\"\"\n",
    "    cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "    num_rows = (len(cols) - 1) // num_cols + 1\n",
    "\n",
    "    # Create a figure with the calculated dimensions\n",
    "    fig, axes = plt.subplots(num_rows, num_cols - 1, figsize=(15, 6 * num_rows))\n",
    "    axes = axes.flatten()  # Flatten the axes array for easier indexing\n",
    "\n",
    "    # Enumerate through columns and create histogram on each subplot\n",
    "    for i, col in enumerate(cols):\n",
    "        sns.histplot(df[col], kde=True, ax=axes[i])\n",
    "        axes[i].set_title(f\"Distribution of {col}\")\n",
    "\n",
    "    # Hide unused subplots (we only need 7 plots)\n",
    "    for i in range(len(cols), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    # Fine-tune spacing between subplots for better readability\n",
    "    plt.subplots_adjust(left=0.05, right=0.95, top=0.9, bottom=0.05, wspace=0.3, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fAbCUibHJS39",
    "outputId": "9bac15bd-da62-40e4-8061-2027709bd121"
   },
   "outputs": [],
   "source": [
    "plot_feature_distributions(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxKt7hdjJS3-"
   },
   "source": [
    "Looking at these distributions, age follows a relatively normal distribution centered around 55-60 years. Resting blood pressure (RestingBP) shows a right-skewed pattern with most values between 120-140 mmHg. Cholesterol exhibits a bimodal distribution with peaks around 200-250 and 400 mg/dL, suggesting potential subgroups in the dataset. Maximum heart rate (MaxHR) appears normally distributed around 150 bpm. Fasting blood sugar (FastingBS) and heart disease status are binary variables with clear 0/1 splits. The Oldpeak measurement (ST depression) is right-skewed with most values close to 0, indicating fewer cases of severe ST depression during exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore the categorical variables, we return to the value counts. We will create some countplots to visualise the potential problems in the balance of our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catcols = ['ChestPainType','FastingBS','RestingECG','ExerciseAngina','ST_Slope']\n",
    "plt.figure(figsize=(10, 6))\n",
    "for col in catcols:\n",
    "    fig = sns.countplot(df,\n",
    "                 x=col, width = 0.5)\n",
    "    fig.set_title(f\"Distribution of categorical variables\")\n",
    "\n",
    "plt.legend(['ChestPainType','FastingBS','RestingECG','ExerciseAngina','ST_Slope'])  \n",
    "fig.set_xlabel('')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(EXPLAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "3bVwhu2lJS3-",
    "outputId": "6fca26b4-c295-4595-f7c6-0e40cd4db509"
   },
   "outputs": [],
   "source": [
    "# Count plot for the target variable\n",
    "sns.countplot(data=df, x=\"HeartDisease\", hue=\"HeartDisease\", palette=\"Set2\", legend=False)\n",
    "plt.title(\"Target Variable Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BX40roUeJS3-"
   },
   "source": [
    "The target variable (HeartDisease) distribution shows a fairly balanced dataset with a slight majority of positive cases (value 1). Out of approximately 900 total samples, around 500 patients have heart disease while about 400 do not. This near-balanced distribution is favorable for machine learning modeling as it reduces the need for sampling techniques to handle class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sns.color_palette(\"tab10\")\n",
    "\n",
    "sns.countplot(data=df,hue=\"Sex\",x=\"HeartDisease\",palette=[s[0],s[6]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the number of males and females in each heart disease category. We see that for healthy patients, we have alot more males than females. We also see that there are very few females with heart disease in our data, these two features may cause concerns on the validity of the machine learning models we create in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfage = df.assign(box_age=pd.cut(df[\"Age\"],\n",
    "                         bins=[0,18,30,45,55,100])).loc[:,[\"box_age\",\"HeartDisease\"]].assign(count_pointer=1).groupby([\"box_age\",\"HeartDisease\"]).agg(count_num=(\"count_pointer\",\"count\"))\n",
    "dfage = dfage.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "s1 = sns.color_palette(\"tab10\")\n",
    "fig =sns.barplot(x=dfage[\"box_age\"],y=dfage[\"count_num\"],hue=dfage[\"HeartDisease\"],palette=[s[2],s[3]])\n",
    "fig.set_xlabel('Age categories')\n",
    "fig.set_ylabel('Count')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we'd expect, the proportion of heart disease cases increases as age does, suggesting older patients are more likely to have heart disease. From this plot We also see our data has lots of patients over the age of 55 with heart disease, which may also affect the validity of our machine larning models, as it will be less trained in predicting heart disease in younger patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2E-9EzeJS3-"
   },
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQbVr87zJS3-"
   },
   "source": [
    "Now we will create a correlation matrix, which will check feature correlations to identify relationships and potential multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "3jk0yW5CJS3-",
    "outputId": "015f38c5-57f8-4b66-c8e3-70d5c7141d3a"
   },
   "outputs": [],
   "source": [
    "# Select only numeric columns for the correlation matrix\n",
    "numeric_df = df.select_dtypes(include=[\"number\"])\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 8))     # fig size\n",
    "sns.heatmap(numeric_df.corr(),  # Calculate correlation matrix\n",
    "            annot=True,         # Display correlation values within cells\n",
    "            cmap=\"coolwarm\",    # Color scheme\n",
    "            fmt=\".2f\")          # formatting the values to 2 decimal places\n",
    "plt.title(\"Feature Correlation Matrix\")     # Setting the title\n",
    "plt.tight_layout()                          # Ensure proper scaping around plot\n",
    "plt.show()                      # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnwDp6mrJS3_"
   },
   "source": [
    "The correlation matrix reveals several important relationships with heart disease. The strongest correlations are with MaxHR (-0.40, negative correlation suggesting lower maximum heart rates are associated with heart disease), and Oldpeak (0.40, positive correlation indicating higher ST depression relates to increased heart disease risk). Age shows a moderate positive correlation (0.28) with heart disease, while cholesterol surprisingly shows a weak negative correlation (-0.23). FastingBS and RestingBP have weak positive correlations. Most features show relatively low correlation with each other, indicating minimal multicollinearity, which is favorable for modeling. Age and MaxHR have a notable negative correlation (-0.38), suggesting maximum heart rate decreases with age as expected biologically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvWtSONVJS3_"
   },
   "source": [
    "## Outlier Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9HD0Jlz0JS3_",
    "outputId": "324b7c8c-5c27-46f5-e6cf-27dcd403f243"
   },
   "outputs": [],
   "source": [
    "# Now we will use boxplot for each numerical feature\n",
    "\n",
    "# Select numerical columns for boxplots\n",
    "cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "\n",
    "# Define subplots grid dimensions\n",
    "num_cols = 4\n",
    "num_rows = (len(cols) - 1) // num_cols + 1      # Number of rows in subplot grid\n",
    "\n",
    "# Create subplot figure with specified dimensions\n",
    "fig, axes = plt.subplots(num_rows, num_cols-1, figsize=(15, 6 * num_rows))\n",
    "\n",
    "# Flatten axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Enumerate through columns and create boxplots on each subplot\n",
    "for i, col in enumerate(cols):\n",
    "    sns.boxplot(data=df[[col]], ax=axes[i],\n",
    "                palette=\"Set3\")\n",
    "    axes[i].set_title(f\"Box plot of {col}\")\n",
    "\n",
    "# Remove unused subplots\n",
    "for i in range(len(cols), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "\n",
    "# Fine-tune spacing between subplots for better readability\n",
    "plt.subplots_adjust(left=0.05, right=0.95, top=0.9, bottom=0.05, wspace=0.3, hspace=0.3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1CebeVCJS3_"
   },
   "source": [
    "Age is centered around 54 years with most patients between 47-60 years. RestingBP shows a median around 130mmHg with several high outliers above 175mmHg. Cholesterol has a median near 230mg/dL with multiple outliers above 500mg/dL. MaxHR is normally distributed around 140bpm with few outliers. Oldpeak shows right-skewed distribution with median near 0.8 and multiple outliers above 4.0. There are notable outliers in RestingBP and Cholesterol that might need attention during preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7VVBP_2JS3_"
   },
   "source": [
    "# Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXadOPDDJS4A"
   },
   "source": [
    "Now we will start implementing our Machine Learning algorithms. Each of us will take one algorithm and we will check the accuracy for each one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EynDXrqEJS4A"
   },
   "source": [
    "# 1. Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETST5NY6JS4A"
   },
   "source": [
    "Random Forest is a machine learning algorithm that combines the output of multiple decision trees to reach a single result. It's particularly effective for both classification and regression problems.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* **Ensemble Learning:** Random Forest uses an ensemble approach, meaning it combines multiple models (decision trees) to make a final prediction.\n",
    "* **Decision Trees:** Each decision tree in the forest makes independent decisions based on a subset of the data and features.\n",
    "* **Randomness:** Randomness is introduced in two ways:\n",
    "    * **Bootstrap Aggregation (Bagging):** Each tree is trained on a random sample of the data with replacement.\n",
    "    * **Feature Randomness:** At each node of a tree, a random subset of features is considered for splitting.\n",
    "* **Prediction:** For classification, the most common class among the trees' predictions is chosen. For regression, the average of the trees' predictions is used.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* **Accuracy:** Random Forest often achieves high accuracy due to its ensemble nature.\n",
    "* **Robustness:** It's less prone to overfitting compared to single decision trees.\n",
    "* **Feature Importance:** It can provide insights into the importance of different features in the data.\n",
    "* **Versatility:** It can handle both numerical and categorical data.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* **Complexity:** Random Forest models can be complex and computationally expensive to train.\n",
    "* **Interpretability:** The ensemble nature can make it difficult to interpret individual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHitZkOVJS4A"
   },
   "outputs": [],
   "source": [
    "# Now we seperate features and the target variable\n",
    "X = df.drop(\"HeartDisease\", axis=1)\n",
    "y = df[\"HeartDisease\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olJm9q50JS4A"
   },
   "outputs": [],
   "source": [
    "# Encode binary categorical variables\n",
    "# We are importing LabelEncoder from sklearn for this process\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "binary_columns = [\"Sex\", \"ExerciseAngina\"]\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in binary_columns:\n",
    "    X[column] = label_encoder.fit_transform(X[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hnd2yDOJS4A"
   },
   "source": [
    "Above code converts categorical binary columns (\"Sex\" and \"ExerciseAngina\") in a DataFrame X into numerical values (0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUz969QnJS4B"
   },
   "outputs": [],
   "source": [
    "# One-hot encode for multi-category categorical variables\n",
    "X = pd.get_dummies(X, columns=[\"ChestPainType\", \"RestingECG\", \"ST_Slope\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DG6q2XmaJS4B"
   },
   "source": [
    "We can see that ChestPainType, RestingECG and ST_Slope has categorical data. That's why we did one-hot encode to convert categorical data to numerical data so that our model can process the data more easily.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "nQZxAoVTJS4B",
    "outputId": "a97b09db-d167-46a3-e26b-ff6307a14ea1"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRVLzK_IJS4C"
   },
   "source": [
    "Now we can see our dataset has all numerical values. Which means now we can start working on our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8ANU01qJS4C"
   },
   "outputs": [],
   "source": [
    "# Now we will split the data to train and test using the train_test_split function from scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgsqgZ1EJS4C"
   },
   "source": [
    "Here we are dividing our dataset into train and test splits. After training our data on train dataset we will test it on our test dataset. Here we are training 80% of the dataset and we will test it on rest which is 20% of the dataset. Here ``X_train``, ``X_test``, ``y_train``, ``y_test`` represent training and testing datasets, where:\n",
    "``X_train`` and ``y_train`` (80% of data) are used for model training,\n",
    "``X_test`` and ``y_tes``t (20% of data) are used for evaluating model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "id": "3HVgOoJ_JS4C",
    "outputId": "e9ed8f3f-dd80-4cb8-ae2c-a2247a1d103b"
   },
   "outputs": [],
   "source": [
    "# Initialize and train the model we\n",
    "# Import the randomForestClassifier from scikitlearn module\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=123)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XYObxX8JS4D"
   },
   "source": [
    "One important to note here is that we have selected ``random_seed=123`` because of reproducibility or we will be getting different accuracy everytime we run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUpjEb00JS4D"
   },
   "source": [
    "## Now we can evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kn-hd5_mJS4D",
    "outputId": "8f311f64-f9a6-41f3-9d8e-f44fa518a41d"
   },
   "outputs": [],
   "source": [
    "# We are calculating the model's accuracy on test data using the .score()\n",
    "test_score = rf_model.score(X_test, y_test)\n",
    "\n",
    "# Print test accuracy with 4 decimal places\n",
    "print(f\"Test Accuracy: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma_H5cnnJS4D"
   },
   "source": [
    "Our Random Forest model got an accuracy of ``0.8641%``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcHLqjLVJS4D"
   },
   "source": [
    "## Confusion Matrix for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "O9_rGSw6JS4D",
    "outputId": "9803719b-b5f3-413e-93ac-5a5e41b2eaf7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Predict the labels for test data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Create confusion matrix comparing true labels and predictions\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # Initialize plot with specified size\n",
    "ConfusionMatrixDisplay(cm).plot(cmap=\"Blues\")   # We have plot the confusion matric with color blue\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6seAk_7JS4E"
   },
   "source": [
    "* **True Positives (95)**: Correctly identified heart disease cases.\n",
    "* **True Negatives (64)**: Correctly identified non-heart disease cases.\n",
    "* **False Positives (17)**: Non-heart disease cases incorrectly classified as heart disease.\n",
    "* **False Negatives (8)**: Heart disease cases missed by the model.\n",
    "\n",
    "The model is effective in detecting heart disease with high true positives and low false negatives, but it has some tendency for false positives, leading to a moderate over-prediction of heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLsKb_u8JS4E"
   },
   "source": [
    "## Classification Report for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CTp5zLaYJS4E",
    "outputId": "6a0e7029-d405-4901-f937-2883524fda03"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict labels for test data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred, target_names=[\"No Heart Disease\", \"Heart Disease\"])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPV7CjBwJS4E"
   },
   "source": [
    "* Precision: 89% for No Heart Disease, 85% for Heart Disease.\n",
    "* Recall: 79% for No Heart Disease, 92% for Heart Disease.\n",
    "* F1-score: Balanced performance with 0.84 for No Heart Disease and 0.88 for Heart Disease.\n",
    "* Accuracy: Overall 86%.\n",
    "* Macro avg: Indicates balanced performance across classes.\n",
    "* Weighted avg: 86%, confirming strong overall accuracy.\n",
    "\n",
    "Hence, this model shows strong performance as it has high recall and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ro-kOfKJS4E"
   },
   "source": [
    "## Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "QVIQ9bdSJS4E",
    "outputId": "80a73357-6e7f-4c9c-bf40-c451cef453ac"
   },
   "outputs": [],
   "source": [
    "# Extract feature importances from Random Forest model\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# Get feature names from DataFrame\n",
    "features = X.columns\n",
    "\n",
    "# Create DataFrame with feature names and importances\n",
    "importance_df = pd.DataFrame({\"Feature\": features, \"Importance\": feature_importances})\n",
    "\n",
    "# Sort DataFrame by importance in descending order\n",
    "importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Initialize plot with specified size\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot feature importances as a bar chart\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=importance_df)\n",
    "plt.title(\"Feature Importance in Random Forest Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhY5qkEeJS4F"
   },
   "source": [
    "From the above feature importance plot figure we can say that Random Forest model shows that `ST_Slope_Up` is the most influential feature in predicting heart disease, followed closely by `MaxHR` and `Oldpeak`. `Cholesterol` and `ST_Slope_Flat` also play significant roles, indicating that heart-related metrics and ST segment changes are strong indicators in the model. Lower-ranked features, like `ChestPainType_TA` and `RestingECG_ST`, contribute less, suggesting they have minimal impact on the model’s predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we reset the dataframe as we implement slightly different pre-processing here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflr = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the EDA we have zero values in the RestingBP and Cholesterol columns which aren't possible. For this algorithm we will handle these 'missing' values. The following code will calculate the median for each column (with the zeros still included), and replace the zero values with the corresponing median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medianC = dflr['Cholesterol'].median()\n",
    "medianRBP = dflr['RestingBP'].median()\n",
    "\n",
    "dflr.loc[dflr['Cholesterol'] == 0, 'Cholesterol'] = medianC\n",
    "dflr.loc[dflr['RestingBP'] == 0, 'RestingBP'] = medianRBP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will transform categorical variables with k levels into k-1 columns, where each column corresponds to a different level of the category. The level that is dropped from the dataframe (using drop_first = True) will be the reference level when we come to comparing coefficients in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Sex', 'ChestPainType', 'FastingBS', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "\n",
    "count = 0\n",
    "for col in cat_cols:\n",
    "  dflr = pd.get_dummies(dflr, columns=[col], dtype = int, drop_first = True)\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we standardise the numerical variables. This bit of code will go through each column with numerical data, subtract the column's mean and divide by it's standard deviation. This will standardise these variables, giving them all mean = 0, variance = 1 (approximately) and again will help us to compare model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "\n",
    "for col in cont_cols:\n",
    "  dflr[col] = (dflr[col] - dflr[col].mean()) / np.sqrt(np.var(dflr[col]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a logistic regression model using statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we format our data. Our response variable (y), is HeartDisease, and our explanatory variables are the remaining columns of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dflr[[i for i in dflr.columns if \"HeartDisease\" not in i]]\n",
    "y = dflr[[i for i in dflr.columns if \"HeartDisease\"  in i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we can now split the data into a set for training the model and a set to test the model. This code will split the data in the same way as before, by utilising the same random state and split size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 123) \n",
    "Train = X_train.assign(HeartDisease = y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I also created a 'Train' dataframe with all the training data, which will come in handy when fitting the model which we will do below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_pilot = smf.logit(\"HeartDisease ~  Age + RestingBP + Cholesterol + MaxHR + Oldpeak + Sex_M + ChestPainType_ATA + ChestPainType_NAP + ChestPainType_TA + FastingBS_1 + RestingECG_Normal + ExerciseAngina_Y + RestingECG_ST + ST_Slope_Flat + ST_Slope_Up\", data = Train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_reg_pilot.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summary of our model gives us alot of valuable information. A useful takeaway is that several variables have p - values > 0.05, making them statistically insignificant at the 5% level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a function that will calculate the accuracy of our logistic regression model, given a specified cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_calc(cutoff, model):\n",
    "  \n",
    "  pd.set_option('future.no_silent_downcasting', True)\n",
    "  y_pred = (model.predict(X_test) > cutoff).replace([True, False], [1,0])\n",
    "\n",
    "  correct = 0\n",
    "\n",
    "  for i in range(len(y_pred)):\n",
    "    if np.array(y_test)[i] == np.array(y_pred)[i]:\n",
    "      correct += 1\n",
    "\n",
    "  return(correct / 184)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_calc(0.5, log_reg_pilot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model with all explanatory variables has accuracy of around 85%. Now lets remove explanatory variables with very large p-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = smf.logit(\"HeartDisease ~  Age + MaxHR + Oldpeak + Sex_M + ChestPainType_ATA + ChestPainType_NAP + ChestPainType_TA + FastingBS_1  + ExerciseAngina_Y  + ST_Slope_Flat + ST_Slope_Up\", data = Train).fit()\n",
    "accuracy_calc(0.5, log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have the same accuracy, but now less explanatory variables included in our model, so we will continue with this reduced model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considering context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we will explore lowering our threshold to reduce the number of false negatives. False negatives are the worst case scenario in our situation as they represent undiagnosed heart disease, meaning ill patients would go without proper treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define a function false_negs to define the number of false negatives produced by a model, given its cutoff threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_negs(cutoff):\n",
    "  \n",
    "  y_pred = (log_reg.predict(X_test) > cutoff).replace([True, False], [1,0])\n",
    "\n",
    "  false_negs = 0\n",
    "\n",
    "  for i in range(len(y_pred)):\n",
    "    if (np.array(y_test)[i] == 1) & (np.array(y_pred)[i] == 0):\n",
    "      false_negs += 1\n",
    "\n",
    "  return(false_negs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will utilise a for loop to determine the number of false negatives and model accuracy for a range of different thresholds, including our optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.arange(0.5,step = 0.002)\n",
    "accuracy = []\n",
    "falsenegs = []\n",
    "\n",
    "for i in vals:\n",
    "\n",
    "   ## Predict class based on this cutoff\n",
    "   y_pred1 = (log_reg.predict(X_test) >= i)\n",
    "\n",
    "   ## Assess the changes\n",
    "   count = false_negs(i)\n",
    "  \n",
    "   falsenegs.append(count)\n",
    "\n",
    "   test_accuracy1 = metrics.accuracy_score(y_test, y_pred1)\n",
    "  \n",
    "   accuracy.append(test_accuracy1)\n",
    "\n",
    "   if i in [0.1,0.15,0.197,0.2,0.3,0.4,0.421,0.5]:\n",
    "      print(f\"For cutoff {i}, test accuracy is {test_accuracy1} and we have {count} false negatives\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function here defines a function to visualise how accuracy changes with number of false negatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade_off_vis():\n",
    "    plt.scatter(falsenegs, accuracy, s = 3)\n",
    "    plt.title('Figure 1: Threshold trade-off', y = 1.07)\n",
    "    plt.xlabel('No. False Negatives')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.xticks(ticks=range(min(falsenegs), max(falsenegs) + 1, 1))\n",
    "    plt.show()    \n",
    "\n",
    "## See optimal accuracy ar around 7 false negatives but we may want to reduce these. Lots of cutoffs have the same false negatives but different accuracy, we can find the max accuracy cutoff.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a function that can be used to find the cutoff point that offers maximum accuracy, given a specified number of false negatives desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def max_accuracy_cutoff(false_neg_count):\n",
    " max_val = float('-inf') \n",
    " a = 0\n",
    " for i in range(len(vals)):\n",
    "    if falsenegs[i] == false_neg_count:  # Check if falsenegs[i] is 1\n",
    "        if vals[i] > max_val:  # Update max_val if the current value is larger\n",
    "            max_val = vals[i]\n",
    "            a = accuracy[i]\n",
    " print(f\"cutoff {max_val} gives us optimal accuracy {a} for {false_neg_count} false negatives\")  \n",
    "\n",
    "max_accuracy_cutoff(3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a function that will take a desired 'maximum percentage of false negatives'. The function will return the cutoff that will maximise test set accuracy, while ensuring that the percentage of false negatives is below the specified number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cutoff_finder(percentage):\n",
    "    false_negs = math.floor((percentage / 100)*len(X_test))\n",
    "    return(max_accuracy_cutoff(false_negs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our optimal threshold was probably not in fact optimal for this context, as using 0.25 preserves accuracy while also decreasing number of false negatives. We will use this for model evaluation. This code illustrates the trade-off between maximising model accuracy and minimising false negatives. It also shows how easy it can be to tweek the model to change it's features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets produce the summary for our new model, using the summary function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_reg.summary())\n",
    "print(accuracy_calc(threshold,log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our model to calculate the probability of heart disease for each observation. The next line of code will extract these probabilities, and convert them into labels for each observation by checking if the probability is above our optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (log_reg.predict(X_test) > threshold).replace([True, False], [1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two blocks of code will convert y_pred and y_test into lists containing only 1 and 0, which will make our model evaluation a lot easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1 = []\n",
    "y_test_final = []\n",
    "y_pred1 = []\n",
    "y_pred_final = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(y_test)):\n",
    "  y_test1.append(np.array(y_test)[i])\n",
    "for i in y_test1:\n",
    "  y_test_final.append(int(i.item() if hasattr(i, \"item\") else i))\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "  y_pred1.append(np.array(y_pred)[i])\n",
    "for i in y_pred1:\n",
    "  y_pred_final.append(int(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['No Heart Disease', 'Heart Disease']\n",
    "print(classification_report(y_test_final,y_pred_final, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a confusion matrix using sklearn's metrics. I will also define a function heatmap_cf to visualise this later in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test_final, y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_cf():\n",
    "  col = sns.color_palette(\"YlOrBr\", as_cmap=True)\n",
    "  sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=col)\n",
    "\n",
    "  ## Edit plot\n",
    "  plt.title('Figure 2: Confusion Matrix for Heart Disease Data', y = 1.07)\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final part of our evaluation will use an ROC curve. The following function roc will produce a plot of the ROC curve plot for our model, as well as computing the area under this curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc():\n",
    "\n",
    "  ## Create ROC curve for logReg\n",
    "  fpr, tpr, thresholds = metrics.roc_curve(y_test,  log_reg.predict(X_test))\n",
    "  auc = metrics.roc_auc_score(y_test, log_reg.predict(X_test))\n",
    "  plt.plot(fpr,tpr,label=\" auc=\"+str(auc))\n",
    "  plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "  plt.title('Figure 3: ROC Curve')\n",
    "  plt.xlabel('Specificity')\n",
    "  plt.ylabel('Sensitivity')\n",
    "  plt.legend(loc=4)\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PT3ou0Q2JS4F"
   },
   "source": [
    "# Project Outcome (10 + 10 marks)\n",
    "\n",
    "_This section should describe the outcome of the project by means of both explanation of the results and by graphical visualisation in the form of graphs, charts or or other kinds of diagram_\n",
    "\n",
    "_The section should begin with a general overview of the results and then have a section for each of the project objectives. For each of these objectives an explanation of more specific results relating to that objective shoud be given, followed by a section presenting some visualisation of the results obtained. (In the case where\n",
    "the project had just one objective, you should still have a section describing\n",
    "the results from a general perspective followed by a section that focuses on\n",
    "the particular objective.)_\n",
    "\n",
    "_The marks for this section will be divided into 10 marks for Explanation\n",
    "and 10 marks for Visualisation. These marks will be awarded for the Project Outcome\n",
    "section as a whole, not for each objective individually. Hence, you do not\n",
    "have to pay equal attention to each. However, you are expected to have a\n",
    "some explanation and visualisation for each. It is suggested you have\n",
    "200-400 words explanation for each objective._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lGQV4QJJS4F"
   },
   "source": [
    "## Overview of Results\n",
    "_Give a general overview of the results (around 200 words)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCwSrlZRJS4F"
   },
   "source": [
    "## Objective 1\n",
    "\n",
    "### Explanation of Results\n",
    "\n",
    "_200-400 words_\n",
    "\n",
    "### Visualisation\n",
    "_The following bar chart gives a vivid representation of the distribution\n",
    "of fridge magnet types, in which the dominance of 'meme' type magnets\n",
    "is dramatically illustrated._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BaaVfgGJS4G"
   },
   "source": [
    "## Objective 2\n",
    "\n",
    "### Explanation of Results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjK6UculJS4G"
   },
   "source": [
    "## Objective 3\n",
    "### Explanation of Results\n",
    "\n",
    "For this objective we fit a logistic regression model using the statsmodels package. The model is trained to use explanatory variables to propose a probability of having heart disease, then use a specified cutoff point to assign a label 0 or 1 to the observation. An important difference in the pre-processing steps we took was that for this algorithm, we replaced the zero values found in the EDA with the corresponding column median. We also standardised the numerical variables, so that we could directly compare their coefficients in our model.\n",
    "\n",
    "We fit the model using our training data, which is the same for all models, and ran the logit summary function. This summary returns a p-value for each of our coefficients, which corresponds to a statistical test with null hypothesis ‘coefficient = 0’. As we can see the p-value for several of our coefficients is large, suggesting there isn’t enough evidence to reject the null hypothesis at the 5% level and they don’t have a significant effect on our response variable in the presence of our other explanatory variables. \n",
    "\n",
    "To test the effects of removing these variables we created a function that takes in a cutoff point, a logistic regression model and calculates the proportion of correctly labeled observations in our test set. From this we confirmed we can greatly simplify our model by removing RestingBP, Cholesterol and RestingECG while maintaining the same level of accuracy. This offers a more efficient model and would greatly reduce the time it takes to gather explanatory variable measurements in practise. We decided to keep Age in, as it is close to significance and would be immediately available in practise anyway.\n",
    "\n",
    "The next step was to consider the context of our project, when doctors are trying to detect heart disease, the worst case scenario is to not detect heart disease in a patient that actually has it, this is known as a false negative. To explore this we created a function that counts the number of false negatives given a cutoff point. We then called the function at different thresholds and saw that generally, accuracy decreases with the number of false negatives. To visualise this we plotted number of false negatives against accuracy, and saw that there was a cutoff point with maximum accuracy for each observed number of false negatives. This inspired us to create a function that found this cutoff point with maximum accuracy, so that we could reduce number of false negatives while maintaining as much model accuracy as possible. As a way to show the real world application of this model, we created a function that can accept a percentage, the proportion of false negatives labels in the test set, and return the cutoff that ensures that percentage isn’t exceeded. This shows how this model can be easily adapted and adjusted when used in practise by healthcare professionals.\n",
    "We chose a cutoff 0.196 using our functions, which provided a test accuracy of 0.82, and only 3 false negatives. \n",
    "\n",
    "The final step was to evaluate this model for comparison. The classification report shows a higher F1-score for the heart disease class, which is due to our threshold being set to favour predicting heart disease cases correctly. The report reveals that we have poor recall for the no heart disease class, showing alot of non heart disease cases were predicted incorrectly. In practise, this may make the process quite costly, as the patient would have to undergo further tests to ensure they do really have heart disease. This is something that requires expert knowledge to discuss in any more detail.\n",
    "\n",
    "The confusion matrix which is visualised (Figure 2), shows the different classifications in our model, the diagonals show correctly classified observations, the bottom left shows false negative cases, while the top right shows the false positive cases. The matrix once again shows the very little amount of false negatives, but high amount of false positives. It also demonstrates the accuracy of the model, with most classifications in the diagonal sections.\n",
    "\n",
    "Lastly we plot the ROC curve (Figure 3), which shows the sensitivity (True positive rate) against the specificity (False positive rate). The dotted line represents the ROC curve for a totally random classifier (AUC = 0.5) while a perfect classifier would have AUC = 1. Our curve hugs the top left corner and has an AUC value close to 1, indicating good model performance. The steep initial increase in the curve shows the model achieves high sensitivity, with a small increase in false positive rate, which illustrates why we could maintain great accuracy, while lowering number of false negatives by using a lower threshold.\n",
    "\n",
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_off_vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dot represents a different cutoff point (or several cutoff points). We see that in general, if we want to reduce false negatives we also have to reduce accuracy. This plot also reveals that there are thresholds with maximum accuracy for each number of false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_cf()\n",
    "roc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad6MFmZgJS4G"
   },
   "source": [
    "## Objective 4\n",
    "### Explanation of Results\n",
    "\n",
    "200-400 Words\n",
    "\n",
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myLtd0cfJS4G"
   },
   "source": [
    "# Conclusion (5 marks)\n",
    "\n",
    "_Your concluding section should be around 200-400 words. It is recommended\n",
    "that you divide it into the following sections._\n",
    "\n",
    "### Achievements\n",
    "_As we had expected, the most popular fridge magnets were of the 'meme' kind.\n",
    "We were surprised that 'smiley' fridge magnets were less common than expected.\n",
    "We conjecture that this is because, although they are apparently very popular,\n",
    "few fridges display more than one smiley. However, 'meme' based magnets can\n",
    "be found in large numbers, even on quite small fridges._\n",
    "\n",
    "### Limitations\n",
    "\n",
    "_The project was limited to a small number of fridge magents, which may not be\n",
    "typical of fridges found in the global fridge magnet ecosystem._\n",
    "\n",
    "### Future Work\n",
    "\n",
    "_In future work we would like to obtain more diverse data and study fridge magnets\n",
    "beyond the limited confines of student accomodation. We hypothesise that there\n",
    "could be a link between fridge magnet types and social class and/or educational\n",
    "achievement._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n"
     ]
    }
   ],
   "source": [
    "print('hey')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
